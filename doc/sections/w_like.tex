% \section{Weighted Likelihood for Sparse Labels}

\subsection{Model Formulation}

One issue with semi-supervised learning in general occurs when the model used for inference is misspecified data is sparsely labelled. In this case, the likelihood can be dominated by the unlabelled data so that the labelled data will not meaningfully affect maximum likelihood estimates \citep{Chapelle:2006,Ren:2020}. One solution described by \citet{Chapelle:2006} is to use a weighted likelihood that gives separate weight to the labelled and unlabelled data sets. Suppose that there are a total of $n$ labelled observations and $m$ unlabelled observations. Then, the weighted log-density of $\Ya_t$ proposed by \citet{Chapelle:2006} is:

\begin{equation}
    \log\left(\fa^{(i)}(\ya_t)\right) = \left(\frac{m+n}{m}(1-\ell_t)(1-\lambda) + \frac{m+n}{n}\ell_t\lambda\right)  \log\left(f^{(i)}(y_t)\right) + \log \left(f^{(i)}(x_t \ell_t|y_t)\right)
    \label{eqn:w_like}
\end{equation}

where $\lambda \in [0,1]$ represents the relative weight given to labelled observations. Setting $\lambda = 0$ throws out all labelled data, setting $\lambda = 1$ throws out all unlabelled data, and setting $\lambda = n/(n+m)$ returns the standard density of from Equation (\ref{eqn:PHMM_emis}). \citet{Chapelle:2006} mention that $\lambda$ can be tuned via cross validation, but cross validation is ``bound to fail if $n$ is very small". The authors then suggest to set $\lambda = 1$ (i.e. only consider labelled observations), and then slowly decrease $\lambda$ until the likelihood surface exhibits significant multi-modality. I will experiment with both cross-validation as well as the method described by \citet{Chapelle:2006} to tune $\lambda$.

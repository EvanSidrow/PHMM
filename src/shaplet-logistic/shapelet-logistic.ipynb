{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.transforms import Bbox\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import circvar\n",
    "from scipy.special import expit\n",
    "from scipy.special import logsumexp\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.signal import convolve\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import importlib\n",
    "import datetime\n",
    "\n",
    "import divebomb\n",
    "import sktime\n",
    "from sktime.transformations.panel.shapelets import ContractedShapeletTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(suppress=True,precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-glasgow",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv('../../Dat/diary/kinematic_data_calibrated_I107_2020_50Hz.csv',\n",
    "                parse_dates = [0], infer_datetime_format = True)\n",
    "\n",
    "# Do some preliminary things\n",
    "df = df.rename(columns={\"p\":\"depth\"})\n",
    "df['elevation'] = -df['depth']\n",
    "\n",
    "# make a backup\n",
    "df_backup = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "fig = df.iloc[::1000].plot(x='Time',y='depth')\n",
    "fig.invert_yaxis()\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record labels\n",
    "forage_events = pd.to_datetime(['2020-08-25 12:33:22',\n",
    "                                '2020-08-25 13:24:15',\n",
    "                                '2020-08-25 16:59:06',\n",
    "                                '2020-08-25 17:07:48',\n",
    "                                '2020-08-25 17:40:50'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-monroe",
   "metadata": {},
   "source": [
    "# Add a couple of varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['d_roll','d_head','d_pitch']] = np.abs(((df[['roll','head','pitch']]\\\n",
    "                                             .rolling(11,center=True)\\\n",
    "                                             .mean()\\\n",
    "                                             .diff(11) + np.pi) % (2*np.pi)) - np.pi)\n",
    "\n",
    "df['VeDBA'] = np.sqrt(df['Aw_1']**2 + df['Aw_2']**2 + df['Aw_3']**2)\n",
    "df['jerk'] = np.abs(((df['VeDBA'].rolling(5,center=True).mean().diff(5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-treasure",
   "metadata": {},
   "source": [
    "# Divide into Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Take the last minute (at depth) of each dive deeper than 20 meters\n",
    "\n",
    "# set parameters\n",
    "surface_thresh = 20\n",
    "dive_duration_thresh = datetime.timedelta(minutes=1.5)\n",
    "between_dive_thresh = datetime.timedelta(seconds=2)\n",
    "h = 4500\n",
    "\n",
    "# add dive numbers\n",
    "df_at_depth = df[df['depth'] >= surface_thresh]\n",
    "df_at_depth['new_dive'] = (df_at_depth['Time'].diff() > between_dive_thresh)\n",
    "df_at_depth['divenum'] = df_at_depth['new_dive'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each dive\n",
    "gbo = df_at_depth.groupby('divenum')\n",
    "\n",
    "# get times\n",
    "time = []\n",
    "\n",
    "# get shapelet candidates\n",
    "depth = []\n",
    "roll = []\n",
    "\n",
    "# get scalar features\n",
    "htv = []\n",
    "hv = []\n",
    "jp = []\n",
    "\n",
    "# get labels\n",
    "labels = []\n",
    "\n",
    "for divenum,group_df in gbo:\n",
    "    \n",
    "    # get the dataframe of the group\n",
    "    group_df = group_df.reset_index().drop('index',1)\n",
    "    \n",
    "    # get start and end of dive\n",
    "    stime = group_df['Time'].iloc[0]\n",
    "    etime = group_df['Time'].iloc[-1]\n",
    "    \n",
    "    # ignore dives shorter than 1 minute (at_depth)\n",
    "    if (etime-stime) < dive_duration_thresh:\n",
    "        print(etime-stime)\n",
    "        continue\n",
    "    \n",
    "    # only take last minute \n",
    "    group_df = group_df.iloc[-h:]\n",
    "    \n",
    "    # time\n",
    "    time.append(group_df['Time'])\n",
    "    \n",
    "    # shaplet features\n",
    "    depth.append(group_df['elevation'])\n",
    "    roll.append(np.abs(group_df['roll']))\n",
    "    \n",
    "    # scalar features\n",
    "    htv.append(group_df['d_head'].sum())\n",
    "    hv.append(circvar(group_df['head'],nan_policy='omit'))\n",
    "    jp.append(group_df['jerk'].max())\n",
    "    \n",
    "    # get the label\n",
    "    label = False\n",
    "    for event in forage_events:\n",
    "        label = (label) or ((stime <= event) and (etime >= event))\n",
    "    labels.append(label)\n",
    "\n",
    "    \n",
    "    if label:\n",
    "        plt.plot(time[-1],depth[-1])\n",
    "        plt.show()\n",
    "        plt.plot(time[-1],roll[-1])\n",
    "        plt.show()\n",
    "        \n",
    "        print(htv[-1])\n",
    "        print(hv[-1])\n",
    "        print(jp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "surface_thresh = 20\n",
    "dive_duration_thresh = datetime.timedelta(minutes=60)\n",
    "between_dive_thresh = datetime.timedelta(seconds=2)\n",
    "h = 3000\n",
    "\n",
    "# divide dataframe into segments\n",
    "df_downsampled = df[['Time','depth']].iloc[::h]\n",
    "df_downsampled['segnum'] = range(len(df_downsampled))\n",
    "\n",
    "# check which segments start at depth\n",
    "df_downsampled['at_depth'] = df_downsampled['depth'] >= surface_thresh\n",
    "\n",
    "# label the dataframe by foraging event\n",
    "df_downsampled['label'] = False\n",
    "for time in forage_events:\n",
    "    df_downsampled['label'] = (df_downsampled['label']) | \\\n",
    "                              ((df_downsampled['Time'] <= time) & \\\n",
    "                              (df_downsampled['Time'] >= time-datetime.timedelta(seconds=h/50)))\n",
    "\n",
    "# combine segments with full dataframe\n",
    "df = df.merge(df_downsampled,how='left')\n",
    "df[['segnum','at_depth','label']] = df[['segnum','at_depth','label']].fillna(method='ffill')\n",
    "\n",
    "# get rid of segments above threshold\n",
    "df = df[df['at_depth']].drop('at_depth',axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-congo",
   "metadata": {},
   "source": [
    "# Format the data for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-corps",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbo = df.groupby('segnum')\n",
    "\n",
    "time = []\n",
    "depth = []\n",
    "pitch = []\n",
    "roll = []\n",
    "head = []\n",
    "labels = []\n",
    "\n",
    "for segnum,group_df in gbo:\n",
    "    \n",
    "    group_df = group_df.reset_index().drop('index',1)\n",
    "    \n",
    "    time.append(group_df['Time'])\n",
    "    depth.append(group_df['elevation'])\n",
    "    pitch.append(group_df['pitch'])\n",
    "    roll.append(np.abs(group_df['roll']))\n",
    "    head.append((group_df['head']-group_df['head'][0]+np.pi)%(2*np.pi)-np.pi)\n",
    "    \n",
    "    labels.append(group_df.label.iloc[0])\n",
    "    \n",
    "    if (group_df.label.iloc[0]) | (segnum < 10):\n",
    "        print(group_df.label.iloc[0])\n",
    "        print('')\n",
    "        print(group_df['d_head'].mean())\n",
    "        print(circvar(group_df['head'],nan_policy='omit'))\n",
    "        #print(group_df['jerk'].max())\n",
    "        group_df.plot(x='Time',y='elevation')\n",
    "        #group_df.plot(x='Time',y=['roll'])\n",
    "        group_df.plot(x='Time',y=['head','d_head'])\n",
    "        #group_df.plot(x='Time',y='VeDBA')\n",
    "        #group_df.plot(x='Time',y='jerk')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "a(group_df['head'][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(group_df['head'][1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.DataFrame({'Time':time,\n",
    "                        'depth':depth,\n",
    "                        'pitch':pitch,\n",
    "                        'roll':roll,\n",
    "                        'head':head})\n",
    "train_y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-season",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negatives = 50\n",
    "\n",
    "inds = np.random.choice(len(train_x),num_negatives,replace=False)\n",
    "inds = np.union1d(inds,np.where(train_y)[0])\n",
    "\n",
    "min_shapelet_length = 25 # 0.5 seconds\n",
    "max_shapelet_length = 250 # 5 seconds  \n",
    "\n",
    "# How long (in minutes) to extract shapelets for.\n",
    "# This is a simple lower-bound initially;\n",
    "# once time is up, no further shapelets will be assessed\n",
    "time_contract_in_mins = 30\n",
    "\n",
    "# The initial number of shapelet candidates to assess per training series.\n",
    "# If all series are visited and time remains on the contract then another\n",
    "# pass of the data will occur\n",
    "initial_num_shapelets_per_case = 10\n",
    "\n",
    "# Whether or not to print on-going information about shapelet extraction.\n",
    "# Useful for demo/debugging\n",
    "verbose = 2\n",
    "\n",
    "sts = []\n",
    "st_dfs = []\n",
    "\n",
    "for col_num,col in enumerate(train_x.columns[1:]):\n",
    "    \n",
    "    print('')\n",
    "    print(col)\n",
    "    print('')\n",
    "\n",
    "    st = ContractedShapeletTransform(\n",
    "        time_contract_in_mins=time_contract_in_mins,\n",
    "        num_candidates_to_sample_per_case=initial_num_shapelets_per_case,\n",
    "        min_shapelet_length = min_shapelet_length,\n",
    "        max_shapelet_length = max_shapelet_length\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    st.fit(train_x.iloc[inds][[col]], train_y[inds])\n",
    "    sts.append(st)\n",
    "    \n",
    "    st_dfs.append(st.transform(train_x.iloc[inds][[col]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data from this transform\n",
    "train_x0 = train_x.iloc[inds]\n",
    "\n",
    "for col_num in range(len(train_x.columns)-1):\n",
    "    \n",
    "    print(train_x.columns[col_num+1])\n",
    "    print('')\n",
    "\n",
    "    # for each extracted shapelet (in descending order of quality/information gain)\n",
    "    for s in sts[col_num].shapelets[0:4]:\n",
    "\n",
    "        # summary info about the shapelet\n",
    "        print(s)\n",
    "\n",
    "        # plot the series that the shapelet was extracted from\n",
    "        plt.plot(train_x0.iloc[s.series_id, col_num+1], \"gray\")\n",
    "\n",
    "        # overlay the shapelet onto the full series\n",
    "        plt.plot(\n",
    "            list(range(s.start_pos, (s.start_pos + s.length))),\n",
    "            train_x0.iloc[s.series_id, col_num+1][s.start_pos : s.start_pos + s.length],\n",
    "            \"r\",\n",
    "            linewidth=3.0,\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "    plt.scatter(st_dfs[col_num][0],st_dfs[col_num][1],c=train_y[inds])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.diff(train_x.iloc[index]['roll']) + np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "((np.diff(train_x.iloc[index]['roll']) + np.pi) % (2*np.pi)) - np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.iloc[index]['roll'].rolling(5,center=True).mean().diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-applicant",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index in [5,6,7,37,43,135,139,153]:\n",
    "\n",
    "    plt.plot(train_x.iloc[index]['Time'],train_x.iloc[index]['depth'])\n",
    "    plt.show()\n",
    "    #plt.plot(train_x.iloc[index]['Time'],train_x.iloc[index]['pitch'])\n",
    "    plt.plot(train_x.iloc[index]['Time'],train_x.iloc[index]['roll'])\n",
    "    plt.plot(train_x.iloc[index]['Time'],np.abs(((train_x.iloc[index]['roll'].diff() + np.pi) % (2*np.pi)) - np.pi))\n",
    "    #plt.plot(train_x.iloc[index]['Time'],train_x.iloc[index]['head'])\n",
    "    #plt.legend(['pitch','roll','depth'])\n",
    "    plt.show()\n",
    "    print(np.mean(np.abs(((train_x.iloc[index]['roll'].rolling(11,center=True).mean().diff(11) + np.pi) % (2*np.pi)) - np.pi)))\n",
    "    print(labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_num in range(4):\n",
    "    print(train_x.columns[col_num+1])\n",
    "    print('')\n",
    "    plt.scatter(st_dfs[col_num][0],st_dfs[col_num][1],c=train_y[inds])\n",
    "    plt.show()\n",
    "    plt.scatter(st_dfs[col_num][2],st_dfs[col_num][3],c=train_y[inds])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "with open('objs.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([sts,st_dfs], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting back the objects:\n",
    "with open('objs.pkl') as f:  # Python 3: open(..., 'rb')\n",
    "    sts,st_dfs = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
